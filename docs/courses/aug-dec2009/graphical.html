---
---
<p><font face="Verdana, Arial, Helvetica, sans-serif" size="2"><b><font color="#cc6600">Probabilistic Graphical Models
</font></b></font></p>
<p align="justify"><font face="Verdana, Arial, Helvetica, sans-serif" size="2"> Graphical models 
 provide a way of modeling high dimensional random structures and have found wide applications.  
 The  popular  Hidden  Markov  Models, Markov  Random  fields.  LDA, fall within this framework.  
 A  graphical model  is  a graph  whose nodes are  random variables.  The  graphical  model  
 formalism  uses  the  structure of the graph  to code independence relations.  The goal  of  
 this course  is  to provide a systematic introduction to the underlying probability  and  
 statistical  issues </font></p>
<p align="justify"><font face="Verdana, Arial, Helvetica, sans-serif" size="2"> Some of  the topics that will be covered are : </font></p>
<!--<p>&nbsp;</p>-->
<!--<p><font face="Verdana, Arial, Helvetica, sans-serif" size="2"><b>References</b></font></p>-->
<ul>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2"> Basic probability and statistics:  Independence, Conditional independence, Multivariate Normal  distribution. 
 Estimation of  parameters, Maximum  Likelihood,  Bayesian  methods.  Exponential families </font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2"> Directed  graphical  models  (Bayesian networks).  D-separation  and  conditional  
Independence.  Markov equivalence.  I-equivalence.  Undirected Graphical Models (Markov Networks).  Markov Networks and Independence.  Gibbs distribution and Markov networks.</font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2"> Gaussian networks, Gaussian Bayesian Networks.  Gaussian markov random fields.
 Hidden Markov models,  Kalman filters, Markov random fields, Generative modeling of  data, LDA.</font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Exact inference in Bayesian networks:  Junction tree algorithm, Belief  Propagation, Forward - Backward algorithm in HMM</font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Approximation inference:  Variational techniques, MCMC  techniques, Gibbs sampling</font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Parameter learning  Learning in fully observed models, multinomial  and multivariate  learning, EM  algorithm</font></li>
  <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Structure learning.  Search over DAGs,  Search over DAG patterns, Model averaging, AIC,  BIC</font></li>
</ul>
<p><font face="Verdana, Arial, Helvetica, sans-serif" size="2"><b>References</b></font></p>
<ul>
 <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Daphne Koller  and Nir Friedman., Probabilistic Graphical Models.</font></li>
 <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Richard Neapolitan., Learning  Bayesian.</font></li>
 <li><font face="Verdana, Arial, Helvetica, sans-serif" size="2">Parts of these two texts will form the core of the course.  As  additional topics are discussed           
 relevant  references  will be  provided.</font></li>
</ul>
